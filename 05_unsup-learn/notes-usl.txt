==================== UNSUPERVISED LEARNING NOTES ====================
Learn a concept based on unlabeled data.

Focus on 3 things:
	- How unsupervised learning fills in the model building gap from the original machine learning workflow.
	- Understand different models developed for unsupervised learning and their relative strengths/weaknesses.
	- Understand the different kinds of conclusions unsupervised learning can generate.
	
	
========== Clustering ==========

k means clustering visualization tool: http://www.naftaliharris.com/blog/visualizing-k-means-clustering/

### K Means in sklearn ###

sklearn.cluster.KMeans
	parameters
	- n_clusters (default 8, you will probably want to change, most important)
	- max_iter: number of times k-means will iterate centroids (default 300, that's probably fine)
	- n_init: number of times the algorithm will run with different centroid seeds (default 10)

# Challenges of k-means
	- Given a fixed training set and fixed number of centroids, will output always be the same? No! Initial conditions matter.
	- Initial conditions of cluster centers can lead to bad clustering.
	
	
========== More Clustering ==========

### Single Linkage Clustering ###
- consider each object a cluster
- define intercluster distance as the distance between the closest two points in the two clusters
- merge two closest clusters
- repeat n-k times to make k clusters

Notes on SLC:
- SLC is deterministic
- Running time is relatively friendly (n points, k clusters) time is O(n^3)

### Soft Clustering ###

Assume the data wa generated by:
	1. Select one of K Gaussians (fixed known variance) uniformly
	2. Sample X_i from that Gaussian
	3. Repeat n times
Task: Find a hypothesis h = <mu1, mu2, ..., mu_k> that maximizes the probability of the data.

# Properties of Expectation Maximizations (EM)
- monotonically non-decreasing likelihood
- does not converge (practically does) (k-means does converge)
- will not diverge
- can get stuck (this happens often, local optima problem. random restart!)
- works with any distribution

### Clustering Properties ###

--Richness--
For any assignment of objects to clusters, there is some distance matrix D such that P_D returns that clustering.

--Scale Invariance--
Scaling distances by a positive value does not change the clustering.

--Consistency--
Shrinking intracluster distances and expanding intercluster distances does not change the clustering. What does this mean?
	- if we put objects already in a cluster closer together and put clusters farther away from each other, the clustering won't change.
	
Impossibility Theorem (Jon Kleinberg): no clustering algorithm can achieve all three of
	- richness
	- scale invariance
	- consistency