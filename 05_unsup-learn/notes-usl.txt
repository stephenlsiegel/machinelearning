==================== UNSUPERVISED LEARNING NOTES ====================
Learn a concept based on unlabeled data.

Focus on 3 things:
	- How unsupervised learning fills in the model building gap from the original machine learning workflow.
	- Understand different models developed for unsupervised learning and their relative strengths/weaknesses.
	- Understand the different kinds of conclusions unsupervised learning can generate.
	
	
========== Clustering ==========

k means clustering visualization tool: http://www.naftaliharris.com/blog/visualizing-k-means-clustering/

### K Means in sklearn ###

sklearn.cluster.KMeans
	parameters
	- n_clusters (default 8, you will probably want to change, most important)
	- max_iter: number of times k-means will iterate centroids (default 300, that's probably fine)
	- n_init: number of times the algorithm will run with different centroid seeds (default 10)

# Challenges of k-means
	- Given a fixed training set and fixed number of centroids, will output always be the same? No! Initial conditions matter.
	- Initial conditions of cluster centers can lead to bad clustering.
	
	
========== More Clustering ==========

### Single Linkage Clustering ###
- consider each object a cluster
- define intercluster distance as the distance between the closest two points in the two clusters
- merge two closest clusters
- repeat n-k times to make k clusters

Notes on SLC:
- SLC is deterministic
- Running time is relatively friendly (n points, k clusters) time is O(n^3)

### Soft Clustering ###

Assume the data wa generated by:
	1. Select one of K Gaussians (fixed known variance) uniformly
	2. Sample X_i from that Gaussian
	3. Repeat n times
Task: Find a hypothesis h = <mu1, mu2, ..., mu_k> that maximizes the probability of the data.

# Properties of Expectation Maximizations (EM)
- monotonically non-decreasing likelihood
- does not converge (practically does) (k-means does converge)
- will not diverge
- can get stuck (this happens often, local optima problem. random restart!)
- works with any distribution

### Clustering Properties ###

--Richness--
For any assignment of objects to clusters, there is some distance matrix D such that P_D returns that clustering.

--Scale Invariance--
Scaling distances by a positive value does not change the clustering.

--Consistency--
Shrinking intracluster distances and expanding intercluster distances does not change the clustering. What does this mean?
	- if we put objects already in a cluster closer together and put clusters farther away from each other, the clustering won't change.
	
Impossibility Theorem (Jon Kleinberg): no clustering algorithm can achieve all three of
	- richness
	- scale invariance
	- consistency
	

========== Feature Scaling ==========
An important step in preprocessing features for some ML algorithms.

A method for rescaling features so they span comparable ranges (like [0,1]).

x' = (x - x_min) / (x_max - x_min)

# minmaxscaler in sklearn

Some algorithms are affected by feature scaling, some aren't.
	- Affected: SVM with RBF kernel, k-means clustering
	- Unaffected: decision tree, linear regression
	

========== Feature Selection ==========
Why do this?

Knowledge Discovery: it is useful to be able to interpret the features.
Curse of Dimensionality: as you add more features, you may need exponentially more data

How hard is this problem?

You have a data set with N features, and you want to transform it to M features where M <= N. There are 2^N
possible subsets, so this problem is very hard. Two general approaches: Filtering and Wrapping.

Filtering:
	+ speed: faster than wrapping
	- speed: features looked at in isolation
	- ignores the learner

Wrapping:
	+ takes into account model bias
	+ takes into account learning algorithm
	- very slow

We could filter using information gain, variance/entropy, independent/non-redundant features.

Wrapping:
	- hill climbing
	- randomized optimization
	- forward search
	- backward search (these weren't well explained)
	
Relevance: 
- X_i is *strongly relevant* if removing it degrades Bayes Optimal Classifier (BOC)
- X_i is *weakly relevant* if:
	- not strongly relevant
	- there exists a subset of features S such that adding X_i to S improves BOC
- Otherwise, X_i is *irrelevant*

Relevance measures the effect on the BOC. Really, it's about information.
Usefulness is about minimizing error given a model/learner.