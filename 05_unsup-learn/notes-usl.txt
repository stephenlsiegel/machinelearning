==================== UNSUPERVISED LEARNING NOTES ====================
Learn a concept based on unlabeled data.

Focus on 3 things:
	- How unsupervised learning fills in the model building gap from the original machine learning workflow.
	- Understand different models developed for unsupervised learning and their relative strengths/weaknesses.
	- Understand the different kinds of conclusions unsupervised learning can generate.
	
	
========== Clustering ==========

k means clustering visualization tool: http://www.naftaliharris.com/blog/visualizing-k-means-clustering/

### K Means in sklearn ###

sklearn.cluster.KMeans
	parameters
	- n_clusters (default 8, you will probably want to change, most important)
	- max_iter: number of times k-means will iterate centroids (default 300, that's probably fine)
	- n_init: number of times the algorithm will run with different centroid seeds (default 10)

# Challenges of k-means
	- Given a fixed training set and fixed number of centroids, will output always be the same? No! Initial conditions matter.
	- Initial conditions of cluster centers can lead to bad clustering.
	
	
========== More Clustering ==========

### Single Linkage Clustering ###
- consider each object a cluster
- define intercluster distance as the distance between the closest two points in the two clusters
- merge two closest clusters
- repeat n-k times to make k clusters

Notes on SLC:
- SLC is deterministic
- Running time is relatively friendly (n points, k clusters) time is O(n^3)

### Soft Clustering ###

Assume the data wa generated by:
	1. Select one of K Gaussians (fixed known variance) uniformly
	2. Sample X_i from that Gaussian
	3. Repeat n times
Task: Find a hypothesis h = <mu1, mu2, ..., mu_k> that maximizes the probability of the data.

# Properties of Expectation Maximizations (EM)
- monotonically non-decreasing likelihood
- does not converge (practically does) (k-means does converge)
- will not diverge
- can get stuck (this happens often, local optima problem. random restart!)
- works with any distribution

### Clustering Properties ###

--Richness--
For any assignment of objects to clusters, there is some distance matrix D such that P_D returns that clustering.

--Scale Invariance--
Scaling distances by a positive value does not change the clustering.

--Consistency--
Shrinking intracluster distances and expanding intercluster distances does not change the clustering. What does this mean?
	- if we put objects already in a cluster closer together and put clusters farther away from each other, the clustering won't change.
	
Impossibility Theorem (Jon Kleinberg): no clustering algorithm can achieve all three of
	- richness
	- scale invariance
	- consistency
	

========== Feature Scaling ==========
An important step in preprocessing features for some ML algorithms.

A method for rescaling features so they span comparable ranges (like [0,1]).

x' = (x - x_min) / (x_max - x_min)

# minmaxscaler in sklearn

Some algorithms are affected by feature scaling, some aren't.
	- Affected: SVM with RBF kernel, k-means clustering
	- Unaffected: decision tree, linear regression
	

========== Feature Selection ==========
Why do this?

Knowledge Discovery: it is useful to be able to interpret the features.
Curse of Dimensionality: as you add more features, you may need exponentially more data

How hard is this problem?

You have a data set with N features, and you want to transform it to M features where M <= N. There are 2^N
possible subsets, so this problem is very hard. Two general approaches: Filtering and Wrapping.

Filtering:
	+ speed: faster than wrapping
	- speed: features looked at in isolation
	- ignores the learner

Wrapping:
	+ takes into account model bias
	+ takes into account learning algorithm
	- very slow

We could filter using information gain, variance/entropy, independent/non-redundant features.

Wrapping:
	- hill climbing
	- randomized optimization
	- forward search
	- backward search (these weren't well explained)
	
Relevance: 
- X_i is *strongly relevant* if removing it degrades Bayes Optimal Classifier (BOC)
- X_i is *weakly relevant* if:
	- not strongly relevant
	- there exists a subset of features S such that adding X_i to S improves BOC
- Otherwise, X_i is *irrelevant*

Relevance measures the effect on the BOC. Really, it's about information.
Usefulness is about minimizing error given a model/learner.


========== Principal Component Analysis (PCA) ==========

Principal Component Analysis: given data, PCA obtains a new coordinate system by translation and rotation only
	- moves center of coordinate system to center of data
	- moves x axis into the principal axis of variation
	- moves other axes into orthogonal, tells you how important axes are

PCA is about using multiple features to make a composite feature that more directly probes the underlying phenomenon.

variance: the willingness/flexibility of an algorithm to learn
variance: technical stats term, roughly the "spread" of the data distribution

The principal component of a dataset is the direction that has the largest variance because it retains
the maximum amount of "information" in the original data.

Projection onto maximal variance minimizes information loss.

PCA
- systematized way to transform input features into principal components
- use PCs as new features
- PCs are directions in data that maximize variance (minimize information loss) when you project/compress down onto them
- more variance of data along a PC, higher that PC is ranked
- first PC has most variance/most information, second PC has most variance/info without overlapping w/ first PC
- max no of PCs = no of input features

# When to use PCA?
- latent features driving the patterns in the data
- dimensionality reduction
	- PCA can help you visualize high dimensional data
	- PCA can help you reduce noise
	- PCA can make other algorithms (regression, classification) work better b/c fewer inputs
	
# PCA Example: face recognition

What makes facial recognition in pictures good for PCA?
	- pictures of faces generally have high input dimensionality (many pixels)
	- faces have general patters that could be captured in a smaller number of dimensions


	
========== Feature Transformation ==========

Feature Transformation: the problem of pre-processing a set of features to create a new (smaller? more compact?) feature
set, while retaining as much (relevant? useful?) information as possible.

We will restrict to linear feature transformation.

Why do feature transformation?

Example: ad hoc information retrieval, aka the Google problem. You have a large number of documents in a 
database somewhere, and you want to retrieve the set of documents that are relevant to some query.

What are our features in the document?
	- words, punctuation
		- problem: lots of words
		- good indicators, though they may have multiple meanings (polysemy, could lead to false positive)
		- many words mean the same thing (synonomy, could lead to false negatives)

Independent Components Analysis
	- PCA is about finding correlation
	- ICA is trying to maximize independence of the new features

PCA: mutually orthogonal, maximal variance, ordered features
ICA: mutually independent, maximal mutual information, bag of features

Some alternatives:

RCA (Random Components Analysis)
	- generates randoms directions
	- it works remarkably well if the next thing you'll do is some sort of classification
	- good way for dealing with curse of dimensionality if you don't think ICA or PCA will work well
	- main advantage: fast
	
LDA (Linear Discriminant Analysis)
	- finds a projection that discriminates based on the label


=== Things We Didn't Cover ===

# Deep Learning / Deep Neural Nets #
A reboot of neural nets.

# Big Data #
The issues that arise when you have a really large amount of data.

# Semi-Supervised Learning #
When you have both labeled and unlabeled data.